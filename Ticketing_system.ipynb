{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a0a756",
   "metadata": {},
   "source": [
    "# Ticket Processing System\n",
    "\n",
    "This notebook demonstrates an NLP-based ticket processing workflow, including data preprocessing, feature engineering, model training, evaluation, entity extraction, and deployment via a Gradio interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577eb96",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b31fedf-d5c5-4050-907f-2aef658258e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   ticket_id      1000 non-null   int64 \n",
      " 1   ticket_text    945 non-null    object\n",
      " 2   issue_type     924 non-null    object\n",
      " 3   urgency_level  948 non-null    object\n",
      " 4   product        1000 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 39.2+ KB\n",
      "   ticket_id                                        ticket_text  \\\n",
      "0          1  Payment issue for my SmartWatch V2. I was unde...   \n",
      "1          2  Can you tell me more about the UltraClean Vacu...   \n",
      "2          3  I ordered SoundWave 300 but got EcoBreeze AC i...   \n",
      "3          4  Facing installation issue with PhotoSnap Cam. ...   \n",
      "4          5  Order #30903 for Vision LED TV is 13 days late...   \n",
      "\n",
      "           issue_type urgency_level            product  \n",
      "0     Billing Problem        Medium      SmartWatch V2  \n",
      "1     General Inquiry           NaN  UltraClean Vacuum  \n",
      "2          Wrong Item        Medium      SoundWave 300  \n",
      "3  Installation Issue           Low      PhotoSnap Cam  \n",
      "4       Late Delivery           NaN      Vision LED TV  \n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "import pandas as pd\n",
    "\n",
    "data= pd.read_excel(\"E:\\\\assignment\\\\ai_dev_assignment_tickets_complex_1000.xls\")\n",
    "data.info()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78988d36-9aab-45e1-810d-fe68efc0f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 876 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   ticket_id      876 non-null    int64 \n",
      " 1   ticket_text    826 non-null    object\n",
      " 2   issue_type     876 non-null    object\n",
      " 3   urgency_level  876 non-null    object\n",
      " 4   product        876 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 41.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.dropna(subset=['issue_type', 'urgency_level'], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994a159",
   "metadata": {},
   "source": [
    "## 2. Importing NLP and ML Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99436b2-c632-4435-85d0-1a7a08ce3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f604fb03-6398-439c-bed0-aa03ce2f249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb77df8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We normalize the text, remove stopwords, perform tokenization and lemmatization, and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e0fd12-f11b-4298-b6d4-d099f9f4bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing (text normalization, Handling missing values, Tokenization, stopword removal, and lemmatization)\n",
    "\n",
    "#Data preprocessing function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "    \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "        # Tokenization\n",
    "        tokens = text.split()\n",
    "    \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "data['cleaned_text'] = data['ticket_text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e3148",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We use TF-IDF vectorization, ticket length, and sentiment score as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da497cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Engineering (TF-IDF Vectorization)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "#additional features\n",
    "#ticket length\n",
    "data['ticket_length'] = data['ticket_text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "#sentiment score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "data['sentiment_score'] = data['ticket_text'].apply(lambda x: sia.polarity_scores(x)['compound'] if isinstance(x, str) else 0)\n",
    "\n",
    "# Combining TF-IDF features with additional features\n",
    "additional_features = data[['ticket_length', 'sentiment_score']]\n",
    "X_combined = hstack([X, additional_features.values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21005fe",
   "metadata": {},
   "source": [
    "## 5. Preparing Target Variables and Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b829fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target variable\n",
    "y_issue = data['issue_type']\n",
    "y_urgency = data['urgency_level']\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train_issue, y_test_issue = train_test_split(X, y_issue, test_size=0.2, random_state = 42)\n",
    "X_train_urgency, X_test_urgency, y_train_urgency, y_test_urgency = train_test_split(X, y_urgency, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662355c9",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation\n",
    "\n",
    "We use a Random Forest Classifier for both issue type and urgency prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b32933f2-9fbc-416b-9360-79a0523e9f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Type Classification Report: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    Account Access       1.00      0.97      0.99        35\n",
      "   Billing Problem       1.00      0.91      0.95        32\n",
      "   General Inquiry       1.00      0.93      0.96        29\n",
      "Installation Issue       1.00      0.92      0.96        24\n",
      "     Late Delivery       0.67      1.00      0.80        20\n",
      "    Product Defect       1.00      0.95      0.97        19\n",
      "        Wrong Item       1.00      0.94      0.97        17\n",
      "\n",
      "          accuracy                           0.94       176\n",
      "         macro avg       0.95      0.94      0.94       176\n",
      "      weighted avg       0.96      0.94      0.95       176\n",
      "\n",
      "Urgency Level Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.32      0.33      0.33        60\n",
      "         Low       0.24      0.22      0.23        54\n",
      "      Medium       0.39      0.39      0.39        62\n",
      "\n",
      "    accuracy                           0.32       176\n",
      "   macro avg       0.31      0.31      0.31       176\n",
      "weighted avg       0.32      0.32      0.32       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model Training and Evaluation\n",
    "\n",
    "# Random Forest Classifier for issue type prediction\n",
    "issue_classifier = RandomForestClassifier()\n",
    "issue_classifier.fit(X_train, y_train_issue)\n",
    "# Random Forest Classifier for urgency level prediction\n",
    "urgency_classifier = RandomForestClassifier()\n",
    "urgency_classifier.fit(X_train_urgency, y_train_urgency)\n",
    "\n",
    "# Model Evaluation for issue type\n",
    "y_pred_issue = issue_classifier.predict(X_test)\n",
    "# Model Evaluation for urgency level\n",
    "y_pred_urgency = urgency_classifier.predict(X_test_urgency)\n",
    "\n",
    "print(\"Issue Type Classification Report: \")\n",
    "print(classification_report(y_test_issue, y_pred_issue))\n",
    "print(\"Urgency Level Classification Report: \")\n",
    "print(classification_report(y_test_urgency, y_pred_urgency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc49cd1",
   "metadata": {},
   "source": [
    "## 7. Entity Extraction\n",
    "\n",
    "We extract product names, dates, and complaint keywords from ticket text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "854c6640-aace-4771-ab99-301423315f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Extraction Function\n",
    "product_names = data['product'].unique().tolist()\n",
    "def extract_entities(ticket_text):\n",
    "    products = re.findall(r'\\b(?:' + '|'.join(map(re.escape, product_names)) + r')\\b', ticket_text, flags=re.IGNORECASE)\n",
    "    dates = re.findall(r'\\b\\d{1,2} \\w+ d{4}\\b', ticket_text)\n",
    "    keywords = ['broken', 'late', 'error']\n",
    "    complaints = [word for word in ticket_text.split() if word.lower() in keywords]\n",
    "    return{\"product_names\": products, \n",
    "           \"dates\": dates, \n",
    "           \"complaints\": complaints\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7a6ef",
   "metadata": {},
   "source": [
    "## 8. Integration Function\n",
    "\n",
    "Combining all steps into a single function for processing new tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ae21c1-9400-4ad0-a010-7b806ef83e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration Function\n",
    "def process_ticket(ticket_text):\n",
    "    # Preprocess the ticket_text\n",
    "    cleaned_text = preprocess_text(ticket_text)\n",
    "    \n",
    "    # Predict issue_type and urgency_level using trained models\n",
    "    issue_type = issue_classifier.predict(vectorizer.transform([cleaned_text]))[0]\n",
    "    urgency_level = urgency_classifier.predict(vectorizer.transform([cleaned_text]))[0]\n",
    "    \n",
    "    # Extract entities\n",
    "    extracted_entities = extract_entities(ticket_text)\n",
    "    \n",
    "    return {\n",
    "        \"predicted_issue_type\": issue_type,\n",
    "        \"predicted_urgency_level\": urgency_level,\n",
    "        \"extracted_entities\": extracted_entities,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac12a80",
   "metadata": {},
   "source": [
    "## 9. Example Usage\n",
    "\n",
    "Test the system on example tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e6b797-aca1-422f-9a48-5bf7d8d93a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predicted_issue_type\": \"Wrong Item\",\n",
      "    \"predicted_urgency_level\": \"Medium\",\n",
      "    \"extracted_entities\": {\n",
      "        \"product_names\": [\n",
      "            \"SmartWatch V2\",\n",
      "            \"PowerMax Battery\"\n",
      "        ],\n",
      "        \"dates\": [],\n",
      "        \"complaints\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_ticket = \"I ordered SmartWatch V2 but got PowerMax Battery instead. My order number is #65084.\"\n",
    "result = process_ticket(example_ticket)\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d14371d-c314-445f-aec4-1c8ea07f327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predicted_issue_type\": \"Late Delivery\",\n",
      "    \"predicted_urgency_level\": \"Medium\",\n",
      "    \"extracted_entities\": {\n",
      "        \"product_names\": [\n",
      "            \"SmartWatch V2\"\n",
      "        ],\n",
      "        \"dates\": [],\n",
      "        \"complaints\": [\n",
      "            \"broken\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_ticket = \"I ordered the SmartWatch V2, but it is broken and the delivery was late.\"\n",
    "result = process_ticket(example_ticket)\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac9f0be-ddc2-4580-9d89-7f747afa2511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predicted_issue_type\": \"Product Defect\",\n",
      "    \"predicted_urgency_level\": \"High\",\n",
      "    \"extracted_entities\": {\n",
      "        \"product_names\": [\n",
      "            \"PowerMax Battery\",\n",
      "            \"EcoBreeze AC\"\n",
      "        ],\n",
      "        \"dates\": [],\n",
      "        \"complaints\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_ticket = \"Both my PowerMax Battery and EcoBreeze AC are lost. Both giving issues. Also, I contacted support on 12 March but got no response.\"\n",
    "result = process_ticket(example_ticket)\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed1c80",
   "metadata": {},
   "source": [
    "## 10. Gradio Interface\n",
    "\n",
    "We use Gradio to deploy the ticket processing system as a web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4781155-60b3-49e3-ba57-a237c6fd810f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "import gradio as gr\n",
    "def gradio_interface(ticket_text):\n",
    "    result = process_ticket(ticket_text)\n",
    "    return json.dumps(result, indent=4)\n",
    "# Create Gradio app\n",
    "iface = gr.Interface(fn=gradio_interface, \n",
    "                     inputs=\"text\", \n",
    "                     outputs=\"json\", \n",
    "                     title=\"Ticket Processing System\",\n",
    "                     description=\"Input raw ticket text to see the predicted issue type, urgency, and extracted entities.\")\n",
    "# Launch the Gradio app\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef8873",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b821f956-7147-46c6-8130-0130698eacdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in e:\\anaconda\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.10.1 (from gradio)\n",
      "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in e:\\anaconda\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in e:\\anaconda\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in e:\\anaconda\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in e:\\anaconda\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.11-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (4.10.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda\\lib\\site-packages (from gradio-client==1.10.1->gradio) (2024.6.1)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.1->gradio)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5.0,>=3.0->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\anaconda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in e:\\anaconda\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\anaconda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\anaconda\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.17.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/54.2 MB 3.4 MB/s eta 0:00:16\n",
      "    --------------------------------------- 1.0/54.2 MB 3.4 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 1.8/54.2 MB 3.7 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 2.6/54.2 MB 3.7 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 3.4/54.2 MB 3.6 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 4.2/54.2 MB 3.7 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 5.5/54.2 MB 4.1 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 7.3/54.2 MB 4.7 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 9.2/54.2 MB 5.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 12.1/54.2 MB 6.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 14.2/54.2 MB 6.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 16.3/54.2 MB 6.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 18.9/54.2 MB 7.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 21.2/54.2 MB 7.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 24.1/54.2 MB 8.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 26.5/54.2 MB 8.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 29.6/54.2 MB 8.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 32.5/54.2 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 35.9/54.2 MB 9.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 39.1/54.2 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 41.9/54.2 MB 10.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 45.1/54.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 49.3/54.2 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  53.2/54.2 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.2/54.2 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.11-py3-none-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 15.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 16.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.6 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Installing collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, groovy, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, typer, safehttpx, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.11.1\n",
      "    Uninstalling tomlkit-0.11.1:\n",
      "      Successfully uninstalled tomlkit-0.11.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 huggingface-hub-0.32.2 orjson-3.10.18 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 typer-0.16.0 uvicorn-0.34.2 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3939394-91d5-49ed-b768-6a4f8b84192e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
